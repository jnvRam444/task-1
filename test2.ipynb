{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Task 2: Chat with Website Using RAG Pipeline<br>\n",
        "**Overview**<BR>\n",
        "The goal is to implement a Retrieval-Augmented Generation (RAG) pipeline that allows users to\n",
        "interact with structured and unstructured data extracted from websites. The system will crawl,\n",
        "scrape, and store website content, convert it into embeddings, and store it in a vector database.\n",
        "Users can query the system for information and receive accurate, context-rich responses\n",
        "generated by a selected LLM.<br><br>\n",
        "**Functional Requirements**<br>\n",
        "**1. Data Ingestion**\n",
        "• Input: URLs or list of websites to crawl/scrape.\n",
        "• Process:\n",
        "o Crawl and scrape content from target websites.\n",
        "o Extract key data fields, metadata, and textual content.\n",
        "o Segment content into chunks for better granularity.\n",
        "o Convert chunks into vector embeddings using a pre-trained embedding model.\n",
        "o Store embeddings in a vector database with associated metadata for e icient\n",
        "retrieval.<br>\n",
        "**2. Query Handling**\n",
        "• Input: User's natural language question.\n",
        "• Process:\n",
        "o Convert the user's query into vector embeddings using the same embedding\n",
        "model.\n",
        "o Perform a similarity search in the vector database to retrieve the most relevant\n",
        "chunks.\n",
        "o Pass the retrieved chunks to the LLM along with a prompt or agentic context to\n",
        "generate a detailed response. <br>\n",
        "**3. Response Generation**\n",
        "• Input: Relevant information retrieved from the vector database and the user query.\n",
        "• Process:\n",
        "o Use the LLM with retrieval-augmented prompts to produce responses with exact\n",
        "values and context.\n",
        "o Ensure factuality by incorporating retrieved data directly into the response."
      ],
      "metadata": {
        "id": "CQzp25b2Fokj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "55RyltaZPX67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ks68bEuFg_e",
        "outputId": "68e53cfe-b0a4-46d4-e88a-c16a7b17ecb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Request error occurred: 403 Client Error: Forbidden for url: https://www.uchicago.edu/\n",
            "Successfully scraped content from (website)\n",
            "Successfully scraped content from (website)\n",
            "Successfully scraped content from (website)\n",
            "Enter your query: distictly uchicago\n",
            "No results found for your query.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def fetch_website_content(url):\n",
        "  \"\"\"Fetches and returns the text content of a website.\"\"\"\n",
        "  try:\n",
        "    response = requests.get(url, timeout=10)\n",
        "    response.raise_for_status()\n",
        "    soup = BeautifulSoup (response.text, 'html.parser')\n",
        "    return soup.get_text(separator='', strip=True)\n",
        "  except requests.exceptions. SSLError as ssl_err:\n",
        "    print(f\"SSL error occurred: {ssl_err}\")\n",
        "  except requests.exceptions.RequestException as req_err:\n",
        "    print(f\"Request error occurred: {req_err}\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "def search_in_content(query, content_dict):\n",
        "  \"\"\"Searches for a query in the scraped content and returns matching results.\"\"\"\n",
        "  matches = []\n",
        "  for url, content in content_dict.items():\n",
        "    if query.lower() in content.lower():\n",
        "      matches.append((url, content))\n",
        "  return matches\n",
        "websites = [\n",
        "    \"https://www.uchicago.edu/\",\n",
        "    \"https://www.washington.edu/\",\n",
        "    \"https://www.stanford.edu/\",\n",
        "    \"https://und.edu/\"\n",
        "    ]\n",
        "\n",
        "scraped_content = {}\n",
        "for website in websites:\n",
        "  content =fetch_website_content(website)\n",
        "  if content:\n",
        "    print(f\"Successfully scraped content from (website)\")\n",
        "    scraped_content [website] = content\n",
        "\n",
        "query_input = input(\"Enter your query: \")\n",
        "search_results = search_in_content(query_input, scraped_content)\n",
        "\n",
        "if search_results:\n",
        "  print(\"\\nResults found:\")\n",
        "  for url, content in search_results:\n",
        "    print(f\"\\nFrom {url}:\\n{content[:200]}...\")\n",
        "else:\n",
        "    print(\"No results found for your query.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4 sentence-transformers faiss-cpu openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1TsWJfwAIIao",
        "outputId": "b6f2a08e-6757-45ec-dad0-a6ab8ef4eb11"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.9.0.post1\n"
          ]
        }
      ]
    }
  ]
}
